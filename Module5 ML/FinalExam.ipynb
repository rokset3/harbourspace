{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Why is it a good idea to normalize data before applying a linear model?\n",
    "#### Answer: \n",
    "To have all of the feature to be in the same scale. When we have different features with different scales, the model's performance may change, since it may think that some of the features \\\n",
    "are more important than the other, due to their high magnitude. \n",
    "### 2 Assume the dataset for binary classification is imbalanced, so 95% of data belong to the first class. How to adjust the classification quality measures, to work with such data? Why?\n",
    "#### Answer:\n",
    "Here we would use F1 score metric to measure the performance of the model. Also we might want to change the loss function, to be adjusted and weighted by the class imbalance, so it would penalize model for miss-classification of a minority class, that missclassification of a majority class. We do that, because the objective of a model is to minimize the loss, and while dataset are imbalanced, having the model which will predict majority class ALWAYS, will have a very small loss, and about 95% accuracy, which is not good for sure. And we might be tricked by this 95% accuracy score, that is why we would apply F1 score, which is a harmonic mean of precision and recall \n",
    "### 3 How to assign a class label for the object in the tree leaf in classification? How to assign a class label for the object in the tree leaf in regression? Does it depend on the information criterion?\n",
    "#### Answer\n",
    "In classification, a class label is typically assigned to an object in a tree leaf by majority vote of the objects that fall into that leaf. For example, if a leaf contains 50 objects and 30 of them are labeled as \"class A\" and 20 are labeled as \"class B\", then the class label for that leaf would be \"class A\". In regression, a class label for an object in a tree leaf is typically assigned by taking the average of the target variable values for all the objects that fall into that leaf. For example, if a leaf contains 50 objects and the target variable values for those objects are [3, 4, 5, 6, 7, 8, 9], then the class label for that leaf would be (3+4+5+6+7+8+9)/7 = 6. The information criterion does not affect classl label assignment, since they are only responsible to deciding the best split point during building of the tree\n",
    "### 4 What is backpropagation? How does it work? E.g. how would gradient propagate through a linear layer? Through ReLU? What will be vector by vector derivative?\n",
    "#### Answer\n",
    "Backpropagation algorithm is an extension of a gradient descent method to a neural network. By applying chain rule and calculating gradients, we would compute for each weight $i$ in a layer $n$ $w_i^n$ we would compute the $\\frac{d\\mathcal{L}(X)}{dw^n_i}$ what is a basically how each weight contributes to a total cost function and what error it gives. Then we use the calculated gradient to update the weight with the respective value. For a linear layer we would calculate the gradient by taking the dot product of an error and an input. For ReLU we have respective derivatives 0 and 1 for negative and non negative values, so the gradient will be 0 for negative input and same as linear layer for positive input. \n",
    "### 5 How does convolutional layer work? What are the kernels (filters) in the covolutional layer? Are they independent?\n",
    "#### Answer\n",
    "Convolutional layers are layers which were a breakthrough in CV field, which allowed many models to surpass the accuracy threshold, while achieving good training speed and parameter size. The convolutional layer benefits from the mathematical operation \"convolution\", here I am not going to talk about it. The idea is, that convolutional layers are used as a feature extractors of an image (and not image too!), and they create a feature map, indicating the presence and the value of the specific feature in specific part of an image. Here kernels are basically filters with some size, and with some weights, which are applied to the matrix and they calculate the presence of features. For each conv layer in a NN, we could have a set of some kernels with different weights, so they would extract different features of the original matrix. And each kernel has its own set of weights which are optimized and changed by training. Kernels are not independent\n",
    "### 6 What is dropout? How does it work in a neural network? Does it change its behaviour on the inference (test) stage?\n",
    "#### Answer\n",
    "Dropout is a technique for regularization in a neural nets. It randomly drops p% of a neurons in a layer, so they are not using during the training at all. This helps to force model to learn different and various independent representations of an input and to prevent overfitting by applying randomness to a model. The difference in traing and inference, that during training we: drop neurons with a probability p. Then we scale the output/weights with a 1/(1-p), so the expected value of an activation from this neurons remains unchanged. In the inference stage, we turn of dropout, so there are no scaling and no neuron dropping. We just use our entire net to achieve better performance   \n",
    "### 7 What is batch normalization? How does it work? How does it affect the learning rate? Does it change its behaviour on the inference (test) stage?\n",
    "#### Answer\n",
    "Batch-norm is a normalization layer, which normalizes the activations of a layer by subtracting their means and dividing by stds of a current batch. This idea allows to stabilize activations and reduce the internal covariate shift across different layers. By stabilizing the activations, it implies, that we may use higher learning rate, since our gradients will be more stable too. So this allows us to train and converge faster. \\\n",
    "Batch-norm does change its behavior during training and inference. The main difference, is that during training we normalize the data by mean and std of the current batch and thus we calculate it for each iteration, while in inference stage the mean and std values are fixed, we do not calculate them, we use average means and std's across all of the batches to normalize the input data. \n",
    "### 8 How does RNN work? Can you combine CNN and RNN? What is the difference between Vanilla RNN and LSTM\\\n",
    "#### Answer\n",
    "RNN - recurrent neural network is a type of nets which were adapted for a sequential data. The idea is that at time $t$, we have an input $x^t$, previous hiddent state from time $t-1$ which is denoted as  $h^{t-1}$, and we use that to calculate the output $y^t$ and next hiddent state $h^t$ to pass for next neurons. But it also may depend on the architecture, but the basic idea of RNN is that. With this implementation, RNN allows us to work with sequential data, and apply some kind of 'memory', which can be helpful when working with sequential data (time-series/text/videoflow). In terms of combining CNN and RNN, yes we can to that. For example we could use CNN as an input layer for a video frame, which will extract the feature representation at time $t$, and then we could feed everything in RNN to get sequential data. Good exampe is OCR, where we extract the digits from the text with CNN, and then try to predict the written word with rnn\\\n",
    "\n",
    "Vanilla RNN and LSTM are both recurrent nets, but they are different in the way, that LSTM solves the vanishing gradient problem. As stated before, RNN has some kind of memory, which allows to remember the sequential data, and how it may affect the output, but given very long sequence, the first inputs may be forgotten. To solve this the LSTM cells were designed with forget, input and output gates, which helps to selectively store the information, which allows to remember long dependencies.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6516f8d802d5d515581c04931032e9d6ff4b607da54a53d6eb72fcfc40b5a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
