{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### HOW TO USE\n",
        "\n",
        "```\n",
        "change data_path in params.yaml\n",
        "run all of the cells below\n",
        "you are great!\n",
        "```\n"
      ],
      "metadata": {
        "id": "1aFI6axpPMJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4gRuM9R7Ypl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323ba018-e038-48f9-8796-b3d42b43edf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/HS/module4-python/final_project/* .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOW TO EDIT params.yaml\n",
        "```\n",
        "data:\n",
        "  data_path: '/content/penguins_lter.csv'           # path to data\n",
        "  imputer_type: 'Simple'                            # available types:['Simple', 'Iterative', 'KNN', 'No'] \n",
        "  impute_strategy: 'mean'                           # Imputing strategy for SimpleImputer.\n",
        "  one_hot: 1                                        # 0 - drop categorical, 1 - one-hot them and include \n",
        "  feature_list: []                                  # list of features we want to include, if empty => include all\n",
        "\n",
        "pipeline:\n",
        "  scaler_type: 'MinMax'                             # available types: ['MinMax', 'Standard']\n",
        "  classifier: 1                                     # integer from 0 to 4, available classifiers: [0: 'LogisticRegression', 1: 'DecisionTree', 2: 'RandomForest', 3: 'HistGBDT', 4: 'LGBM']\n",
        "  n_polynomial_features: 2                          # degree of polynomial features. if less than 1 => do not include polynomial features \n",
        "  random_state: 42\n",
        "  n_components: 0                                   #n_components for pca. if 0 => pca is not included\n",
        "  test_size: 0.2\n",
        "\n",
        "\n",
        "```                    \n",
        "\n"
      ],
      "metadata": {
        "id": "FnuDVZ_i7l57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir final_project"
      ],
      "metadata": {
        "id": "ByLo6DbpWvme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3813ed46-084d-459a-b369-7c44fd6fdf37"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘final_project’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_project/params.yaml\n",
        "\n",
        "data:\n",
        "  data_path: '/content/penguins_lter.csv'            \n",
        "  imputer_type: 'KNN'                             \n",
        "  impute_strategy: 'median'                           \n",
        "  one_hot: 1                                         \n",
        "  feature_list: []                                 \n",
        "\n",
        "pipeline:\n",
        "  scaler_type: 'Standard'                             \n",
        "  classifier: 1              \n",
        "  n_polynomial_features: 3                          \n",
        "  random_state: 42\n",
        "  n_components: 0\n",
        "  test_size: 0.2\n",
        "  params: {}\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf11ioy0Wyfp",
        "outputId": "8bde5762-47e0-4aa4-b14c-89d5e45e94d3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_project/params.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_project/settings.py\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "ROOT_DIR = os.path.join(os.getcwd(), 'final_project') \n",
        "BASE_PARAMS_DIR = os.path.join(ROOT_DIR, 'params.yaml') \n",
        "\n",
        "with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "    cfg = yaml.safe_load(fd)\n",
        "\n",
        "EXPERIMENT_DIR = os.path.join(ROOT_DIR, 'data')\n",
        "request_number = 1\n",
        "PATH = os.path.join(EXPERIMENT_DIR, f'exp{request_number}')\n",
        "\n",
        "while os.path.exists(PATH):\n",
        "    request_number +=1\n",
        "    PATH = os.path.join(EXPERIMENT_DIR, f'exp{request_number}')\n",
        "\n",
        "DATA_PATH = PATH\n",
        "PARAMS_DIR = os.path.join(DATA_PATH, 'experiment_params.yaml')\n"
      ],
      "metadata": {
        "id": "LBTj6KKT9uvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336aaffe-6064-47b7-89a6-cd4a4ef85f58"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_project/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_project/utils.py \n",
        "import pandas as pd\n",
        "import yaml\n",
        "import numpy as np\n",
        "from settings import BASE_PARAMS_DIR\n",
        "from data_transforms import DummyTransformer\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator, IterativeImputer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
        "from sklearn.decomposition import PCA \n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(BASE_PARAMS_DIR=BASE_PARAMS_DIR):\n",
        "  \n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "      cfg = yaml.safe_load(fd)\n",
        "    \n",
        "    feature_list = cfg.get('data')['feature_list']\n",
        "    cat_cols = ['species', 'island']    #categorical columns. will drop them, if one_hot == 0, or will do one_hot encodings if one_hot == 1\n",
        "    \n",
        "    drop_cols = ['studyName',         #this columns will be dropped in any ways... (useless/noninformative)\n",
        "                'Sample Number',\n",
        "                'Region',             #single unique value for all datapoints \n",
        "                'Individual ID',\n",
        "                'Comments',\n",
        "                'Date Egg',\n",
        "                'Stage']              #single unique value for all datapoints\n",
        "\n",
        "    \n",
        "    # preprocessing data\n",
        "    data = pd.read_csv(cfg.get('data')['data_path'])\n",
        "    data = data.drop(drop_cols, axis=1)             \n",
        "    data.rename(columns = {'Delta 15 N (o/oo)': 'delta15n',\n",
        "                          'Delta 13 C (o/oo)': 'delta13c',\n",
        "                          'Culmen Length (mm)': 'culmen_len',\n",
        "                          'Culmen Depth (mm)': 'culmen_dep',\n",
        "                          'Flipper Length (mm)': 'flipper_len',\n",
        "                          'Body Mass (g)': 'mass',\n",
        "                          'Sex': 'target',}, inplace = True)\n",
        "\n",
        "    data.columns = map(str.lower, data.columns)\n",
        "    data['clutch completion'] = data['clutch completion'].map({'Yes': 1,\n",
        "                                                                'No': 0})\n",
        "    \n",
        "    data['target'] = data['target'].map({'MALE':1,\n",
        "                                        'FEMALE':0})\n",
        "    #taking specified subset of features written in params.yaml\n",
        "    if len(cfg.get('data')['feature_list'])>1:\n",
        "      data = data[feature_list]\n",
        "\n",
        "    #checking if we want to process categorical features\n",
        "    if cfg.get('data')['one_hot'] == 1:\n",
        "      one_hots = pd.get_dummies(data[cat_cols])\n",
        "      data = pd.concat([data, one_hots],axis=1)\n",
        "    \n",
        "    data = data.drop(cat_cols, axis=1)\n",
        "\n",
        "    if cfg.get('data')['imputer_type'] == 'No':\n",
        "      data = data.dropna()\n",
        "    \n",
        "    data = data.dropna(subset = ['target'])\n",
        "\n",
        "    return data\n",
        "\n",
        "def impute_data(data, BASE_PARAMS_DIR=BASE_PARAMS_DIR):\n",
        "\n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "        cfg = yaml.safe_load(fd)\n",
        "    data = data.copy()\n",
        "    cols = data.columns.values\n",
        "    imp_strategy = cfg.get('data')['impute_strategy']\n",
        "    imputer_type = cfg.get('data')['imputer_type']\n",
        "    random_state = cfg.get('pipeline')['random_state']\n",
        "    if imputer_type == 'Simple':\n",
        "      imputer = SimpleImputer(missing_values=np.nan, strategy=imp_strategy)\n",
        "\n",
        "    if imputer_type == 'Iterative':\n",
        "      imputer = IterativeImputer(missing_values=np.nan, random_state=random_state)\n",
        "\n",
        "    if imputer_type == 'KNN':\n",
        "      imputer = KNNImputer()\n",
        "\n",
        "    if imputer_type == 'No':\n",
        "      imputer = DummyTransformer()\n",
        "\n",
        "    else:\n",
        "      imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    \n",
        "    data = pd.DataFrame(imputer.fit_transform(data),\n",
        "                        columns = cols)\n",
        "    return data\n",
        "\n",
        "def make_pipeline(BASE_PARAMS_DIR=BASE_PARAMS_DIR):\n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "        cfg = yaml.safe_load(fd)\n",
        "\n",
        "\n",
        "    scaler_type = cfg.get('pipeline')['scaler_type']\n",
        "    degree = cfg.get('pipeline')['n_polynomial_features']\n",
        "    clf = cfg.get('pipeline')['classifier']\n",
        "    n_components = cfg.get('pipeline')['n_components']\n",
        "    random_state = cfg.get('pipeline')['random_state']\n",
        "\n",
        "    \n",
        "    params = cfg.get('pipeline')['params']\n",
        "    params['random_state'] = random_state\n",
        "\n",
        "    #determining scaler\n",
        "    if scaler_type == 'MinMax':\n",
        "      scaler = MinMaxScaler()\n",
        "\n",
        "    if scaler_type == 'Standard':\n",
        "      scaler = StandardScaler()  \n",
        "\n",
        "    #determining polynomial degree\n",
        "    if degree <2:\n",
        "      polynomial_features = DummyTransformer()\n",
        "\n",
        "    else:\n",
        "      polynomial_features = PolynomialFeatures(degree=degree) \n",
        "\n",
        "    #determining pca\n",
        "    if n_components > 0:\n",
        "      pca = PCA(n_components, random_state=random_state)\n",
        "\n",
        "    else:\n",
        "      pca = DummyTransformer()\n",
        "\n",
        "\n",
        "\n",
        "    #determining classifier\n",
        "    if clf == 0:\n",
        "      clf = LogisticRegression(**params)\n",
        "\n",
        "    elif clf == 1:\n",
        "      clf = DecisionTreeClassifier(**params)\n",
        "\n",
        "    elif clf == 2:\n",
        "      clf = RandomForestClassifier(**params)\n",
        "\n",
        "    elif clf == 3:\n",
        "      clf = HistGradientBoostingClassifier(**params)\n",
        "\n",
        "    elif clf == 4:\n",
        "      clf = LGBMClassifier(**params)\n",
        "\n",
        "\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        ('polynomial-features', polynomial_features),\n",
        "        ('scaler', scaler),\n",
        "        ('pca', pca),\n",
        "        ('classifier', clf),\n",
        "    ])\n",
        "\n",
        "    return pipe\n"
      ],
      "metadata": {
        "id": "id0U4QQI9abG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5532e306-512d-43e8-9964-ba7c400f83c9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_project/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_project/data_transforms.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class DummyTransformer(TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      return X\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytc-VZUP4mB9",
        "outputId": "fc5409e0-89bf-4c4e-d5ac-14a76702c7bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_project/data_transforms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lCdb60bYyCAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_project/train.py\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "from settings import BASE_PARAMS_DIR, DATA_PATH, PARAMS_DIR, EXPERIMENT_DIR\n",
        "from utils import preprocess_data, impute_data, make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score, adjusted_mutual_info_score, roc_auc_score, log_loss, f1_score\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "      cfg = yaml.safe_load(fd)\n",
        "\n",
        "    if not os.path.exists(EXPERIMENT_DIR):\n",
        "      os.mkdir(EXPERIMENT_DIR)\n",
        "\n",
        "    if not os.path.exists(DATA_PATH):\n",
        "      os.mkdir(DATA_PATH)\n",
        "\n",
        "    random_state = cfg.get('pipeline')['random_state']\n",
        "    test_size = cfg.get('pipeline')['test_size']\n",
        "\n",
        "    data = preprocess_data()\n",
        "    data = impute_data(data)\n",
        "    pipe = make_pipeline()\n",
        "\n",
        "    train_data, test_data = train_test_split(data, test_size=test_size)\n",
        "    X_train, y_train = train_data.drop('target',axis=1), train_data.target\n",
        "    X_val, y_val = test_data.drop('target',axis=1), test_data.target\n",
        "    print(pipe)\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_val)\n",
        "    preds_proba = pipe.predict_proba(X_val)\n",
        "\n",
        "    metrics_dict = {\n",
        "    'precision_score': float(precision_score(y_pred=preds, y_true=y_val)),\n",
        "    'recall_score': float(recall_score(y_pred=preds, y_true=y_val)),\n",
        "    'accuracy_score': float(accuracy_score(y_pred=preds, y_true=y_val)),\n",
        "    'adjusted_mutual_info_score': float(adjusted_mutual_info_score(y_val, preds)),\n",
        "    'roc_auc_score': float(roc_auc_score(y_val, preds_proba[:,1])),\n",
        "    'log_loss': float(log_loss(y_val, preds_proba[:,1])),\n",
        "    'f1_score': float(f1_score(y_pred=preds, y_true=y_val)),\n",
        "    }\n",
        "    \n",
        "    with open(DATA_PATH + '/report.yaml', 'w') as f:\n",
        "       yaml.dump(metrics_dict, f, default_flow_style=False)\n",
        "    \n",
        "    joblib.dump(pipe, DATA_PATH + '/model.pkl')\n",
        "\n",
        "    with open(PARAMS_DIR, 'w') as f:\n",
        "      documents = yaml.dump(cfg, f)\n",
        "\n",
        "    print(f'results saved at {DATA_PATH}')\n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rv55u__BHBd",
        "outputId": "def00ab4-f032-4d05-cd1f-b5718abb123a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_project/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python final_project/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFyhzI9xEaSf",
        "outputId": "890aad0d-f39b-4878-d154-c22c9815f5cb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('polynomial-features', PolynomialFeatures(degree=3)),\n",
            "                ('scaler', StandardScaler()),\n",
            "                ('pca',\n",
            "                 <data_transforms.DummyTransformer object at 0x7f9289e1aa00>),\n",
            "                ('classifier',\n",
            "                 DecisionTreeClassifier(max_depth=4, random_state=42))])\n",
            "results saved at /content/final_project/data/exp14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/final_project.zip /content/final_project"
      ],
      "metadata": {
        "id": "_C29kXjGLZaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c67c52-8914-4dba-e3e2-1b6fcd88cdd0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/final_project/ (stored 0%)\n",
            "  adding: content/final_project/data/ (stored 0%)\n",
            "  adding: content/final_project/data/exp1/ (stored 0%)\n",
            "  adding: content/final_project/data/exp1/model.pkl (deflated 52%)\n",
            "  adding: content/final_project/data/exp1/report.yaml (deflated 37%)\n",
            "  adding: content/final_project/data/exp1/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp10/ (stored 0%)\n",
            "  adding: content/final_project/data/exp10/model.pkl (deflated 82%)\n",
            "  adding: content/final_project/data/exp10/report.yaml (deflated 40%)\n",
            "  adding: content/final_project/data/exp10/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp4/ (stored 0%)\n",
            "  adding: content/final_project/data/exp4/model.pkl (deflated 27%)\n",
            "  adding: content/final_project/data/exp4/report.yaml (deflated 40%)\n",
            "  adding: content/final_project/data/exp4/experiment_params.yaml (deflated 31%)\n",
            "  adding: content/final_project/data/exp3/ (stored 0%)\n",
            "  adding: content/final_project/data/exp3/model.pkl (deflated 26%)\n",
            "  adding: content/final_project/data/exp3/report.yaml (deflated 37%)\n",
            "  adding: content/final_project/data/exp3/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp7/ (stored 0%)\n",
            "  adding: content/final_project/data/exp7/model.pkl (deflated 33%)\n",
            "  adding: content/final_project/data/exp7/report.yaml (deflated 36%)\n",
            "  adding: content/final_project/data/exp7/experiment_params.yaml (deflated 40%)\n",
            "  adding: content/final_project/data/exp13/ (stored 0%)\n",
            "  adding: content/final_project/data/exp13/model.pkl (deflated 39%)\n",
            "  adding: content/final_project/data/exp13/report.yaml (deflated 38%)\n",
            "  adding: content/final_project/data/exp13/experiment_params.yaml (deflated 31%)\n",
            "  adding: content/final_project/data/exp5/ (stored 0%)\n",
            "  adding: content/final_project/data/exp5/model.pkl (deflated 33%)\n",
            "  adding: content/final_project/data/exp5/report.yaml (deflated 39%)\n",
            "  adding: content/final_project/data/exp5/experiment_params.yaml (deflated 40%)\n",
            "  adding: content/final_project/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/final_project/data/exp8/ (stored 0%)\n",
            "  adding: content/final_project/data/exp8/model.pkl (deflated 37%)\n",
            "  adding: content/final_project/data/exp8/report.yaml (deflated 40%)\n",
            "  adding: content/final_project/data/exp8/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp2/ (stored 0%)\n",
            "  adding: content/final_project/data/exp2/model.pkl (deflated 26%)\n",
            "  adding: content/final_project/data/exp2/report.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp2/experiment_params.yaml (deflated 33%)\n",
            "  adding: content/final_project/data/baseline/ (stored 0%)\n",
            "  adding: content/final_project/data/baseline/model.pkl (deflated 37%)\n",
            "  adding: content/final_project/data/baseline/report.yaml (deflated 43%)\n",
            "  adding: content/final_project/data/baseline/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp11/ (stored 0%)\n",
            "  adding: content/final_project/data/exp14/ (stored 0%)\n",
            "  adding: content/final_project/data/exp14/model.pkl (deflated 40%)\n",
            "  adding: content/final_project/data/exp14/report.yaml (deflated 38%)\n",
            "  adding: content/final_project/data/exp14/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp9/ (stored 0%)\n",
            "  adding: content/final_project/data/exp9/model.pkl (deflated 69%)\n",
            "  adding: content/final_project/data/exp9/report.yaml (deflated 41%)\n",
            "  adding: content/final_project/data/exp9/experiment_params.yaml (deflated 32%)\n",
            "  adding: content/final_project/data/exp6/ (stored 0%)\n",
            "  adding: content/final_project/data/exp6/model.pkl (deflated 38%)\n",
            "  adding: content/final_project/data/exp6/report.yaml (deflated 36%)\n",
            "  adding: content/final_project/data/exp6/experiment_params.yaml (deflated 39%)\n",
            "  adding: content/final_project/data/exp12/ (stored 0%)\n",
            "  adding: content/final_project/data/exp12/model.pkl (deflated 42%)\n",
            "  adding: content/final_project/data/exp12/report.yaml (deflated 37%)\n",
            "  adding: content/final_project/data/exp12/experiment_params.yaml (deflated 30%)\n",
            "  adding: content/final_project/train.py (deflated 63%)\n",
            "  adding: content/final_project/utils.py (deflated 70%)\n",
            "  adding: content/final_project/data_transforms.py (deflated 41%)\n",
            "  adding: content/final_project/__pycache__/ (stored 0%)\n",
            "  adding: content/final_project/__pycache__/data_transforms.cpython-38.pyc (deflated 44%)\n",
            "  adding: content/final_project/__pycache__/settings.cpython-38.pyc (deflated 27%)\n",
            "  adding: content/final_project/__pycache__/utils.cpython-38.pyc (deflated 43%)\n",
            "  adding: content/final_project/params.yaml (deflated 56%)\n",
            "  adding: content/final_project/settings.py (deflated 52%)\n"
          ]
        }
      ]
    }
  ]
}