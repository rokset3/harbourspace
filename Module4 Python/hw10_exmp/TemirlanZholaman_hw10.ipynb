{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### HOW TO UTILIZE:\n",
        "make changes in params.yaml based on experiment you want to conduct \n",
        "then run\n",
        "\n",
        "```\n",
        "from hw10_exp.my_utils import get_data \n",
        "from hw10_exp.custom_pipeline import split_data \n",
        "from hw10_exp.train import train_clf \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()\n",
        "```\n",
        "results, reports, datasets and configs are stored in \n",
        "hw10_exp/data/exp foldreds"
      ],
      "metadata": {
        "id": "0xZK7SEDOtZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOW TO EDIT params.yaml\n",
        "\n",
        "```\n",
        "data:\n",
        "  data_path: '/content'                                # insert your path to folder, where all datasets are stored\n",
        "  main_data_path: '/content/movies_metadata.csv'       # path to main dataset\n",
        "  joined_data_name: 'None'                             # additional data you want to join to main_data\n",
        "  additional_data: \n",
        "      {\"credits\":'/content/credits.csv',               \n",
        "      \"ratings\":'/content/ratings.csv',\n",
        "      \"ratings_small\":'/content/ratings_small.csv'}\n",
        "  use_categorical: 0                                   # this applies one_hot encodings for categorical features (such as status and language)\n",
        "  included_features: ['adult',                         # here we can choose the set of features we want to include                           \n",
        "                     'budget',                         # this is full list, feel free to delete any\n",
        "                     'overview',                       # if this textual feature is not present, we will just simply run default pipeline (scaler->pca->clf)\n",
        "                     'popularity', \n",
        "                     'revenue', \n",
        "                     'runtime',\n",
        "                     'video', \n",
        "                     'vote_count', \n",
        "                     'years', \n",
        "                     'target']\n",
        "experiment:                                           # feature extraction part\n",
        "  preprocessing_type: 4                               # 'overview' feature preprocessing according to the HW5 (use 1, 2, 3, 4 for changing pipeline) \n",
        "  pca: 0                                              # include PCA transformation (0/1)\n",
        "  n_components: 4                                     # if PCA included, how many components to include (any int)\n",
        "  random_state: 22                                    # random state to reproduce\n",
        "  test_size: 0.2                                      # test_size of splitting the data\n",
        "  stratify: 1                                         # to statify train_test_split according to target column\n",
        "  classifier: 'RandomForest'                          # available options: 'RandomForest', 'LogisticRegression', 'DecisionTree'. Logistic Regression by default\n",
        "params_dectree:                                       # enter valid hyperparameters for Decision Tree Classifier\n",
        "    {\"criterion\": 'gini',\n",
        "    \"splitter\": 'best',\n",
        "    \"ccp_alpha\": 0.0}\n",
        "\n",
        "params_logreg:                                        # enter valid hyperparameters for Logistic Regression Classifier\n",
        "    {\"n_estimators\": 100,\n",
        "    \"penalty\": 'l2',\n",
        "    }\n",
        "\n",
        "params_randomforest:                                  # enter valid hyperparameters for Random Forest Classifier\n",
        "  {\"n_estimators\": 100,}\n",
        "```"
      ],
      "metadata": {
        "id": "CM-afGV2P4vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some experiments runs"
      ],
      "metadata": {
        "id": "1u2VES44TelA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        #exp1 run\n",
        "from hw10_exp.custom_pipeline import split_data               #pca 10 features on 1-gram vectorizer with logreg\n",
        "from hw10_exp.train import train_clf \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HYX_6IXTiEm",
        "outputId": "bb245ae7-85ab-4aa2-d043-8019b5dbfd36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-4-cfbce4c311f7>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp2 run\n",
        "from hw10_exp.custom_pipeline import split_data               # testing 2nd pipeline with NP extraction, PCA on 20 features with LogReg\n",
        "from hw10_exp.train import train_clf                          # \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U5nVzpBUfLC",
        "outputId": "8ccd311a-7d90-4e6e-c55b-c07ae73424b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-1-7e31ba46bd92>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting n-grams for NN tokes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp3 run No PCA on numerical and categorical features (no text) \n",
        "from hw10_exp.custom_pipeline import split_data               # \n",
        "from hw10_exp.train import train_clf                          # \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INyihg0BXsRC",
        "outputId": "7799eca2-83bb-451d-d12b-f8db7ce77af8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-1-0bb2de138171>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp4 run\n",
        "from hw10_exp.custom_pipeline import split_data               # Random Forest on ngrams without pca\n",
        "from hw10_exp.train import train_clf                          # So far best\n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcsh0ukcYQ32",
        "outputId": "4815ee86-4c4b-484c-bb8e-345bca76c5db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-1-f0ce5fc6b260>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp5 run\n",
        "from hw10_exp.custom_pipeline import split_data               # here i dropped overview,and applied one-hot encoding for cat features without pca on Random Forest.\n",
        "from hw10_exp.train import train_clf                          # \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpIO20Z9bq6D",
        "outputId": "c91cc0b3-75e2-4719-9ebf-e937b628a551"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-1-660e10d0f059>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp5 run\n",
        "from hw10_exp.custom_pipeline import split_data               # here i dropped overview,and applied one-hot encoding for cat features without pca.\n",
        "from hw10_exp.train import train_clf                          # \n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "id": "8Prd38RNcsE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hw10_exp.my_utils import get_data                        # exp6 run\n",
        "from hw10_exp.custom_pipeline import split_data               # without pca, with categorical features and NN extraction on random forest with 300 trees.\n",
        "from hw10_exp.train import train_clf                          # best one so far\n",
        "\n",
        "data = get_data()\n",
        "split_data(data)\n",
        "train_clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eka4ASinkr1N",
        "outputId": "71a218ab-0d9b-4d1b-e1a3-e3b53c332e73"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hw10_exp\n",
            "/content/hw10_exp/params.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "<ipython-input-1-3449719e02dd>:5: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = get_data()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting n-grams for NN tokes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fz9BRrUKVFO"
      },
      "source": [
        "#### HW 1: Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1pcuK5bKVFR"
      },
      "source": [
        "https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaT1gE67fGvA",
        "outputId": "bf0cdb8c-ef37-47b3-91d0-aa1215a9d053"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fj6k8f_s5d0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp Drive/MyDrive/HS/module4-python/* .\n"
      ],
      "metadata": {
        "id": "2GahPZ1pBZ2u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HW 2.1: Costruct target variable \"above average user rating\" at movies_metadata.csv  Create a column from column vote_average as 1 if it is greather than media, and 0 if it is not."
      ],
      "metadata": {
        "id": "4SWPai08-DBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#processing the data\n",
        "import pandas as pd\n",
        "main_data = pd.read_csv('movies_metadata.csv')\n",
        "main_data[\"id\"] = pd.to_numeric(main_data[\"id\"], errors=\"coerce\", downcast=\"integer\")\n",
        "main_data[\"budget\"] = pd.to_numeric(main_data[\"budget\"], errors=\"coerce\")\n",
        "main_data[\"popularity\"] = pd.to_numeric(main_data[\"popularity\"], errors = \"coerce\")\n",
        "main_data['overview'] = main_data['overview'].astype(str)\n",
        "main_data = main_data.drop('belongs_to_collection', axis=1)\n",
        "main_data = main_data[~main_data.status.isna()]\n",
        "main_data.set_index('id', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "#getting median of votes\n",
        "median_vote = main_data.vote_average.median()\n",
        "\n",
        "#creating 'target' column\n",
        "main_data['target'] = (main_data['vote_average']>median_vote).astype(int)\n",
        "print(f'Median vote is {median_vote}')\n",
        "display(main_data.head(1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "UrlTUa0zN-RK",
        "outputId": "5944cae7-0910-4f57-ac90-52510782dd0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median vote is 6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       adult      budget                                             genres  \\\n",
              "id                                                                            \n",
              "862.0  False  30000000.0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
              "\n",
              "                                   homepage    imdb_id original_language  \\\n",
              "id                                                                         \n",
              "862.0  http://toystory.disney.com/toy-story  tt0114709                en   \n",
              "\n",
              "      original_title                                           overview  \\\n",
              "id                                                                        \n",
              "862.0      Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
              "\n",
              "       popularity                       poster_path  ...      revenue runtime  \\\n",
              "id                                                   ...                        \n",
              "862.0   21.946943  /rhIRbceoE9lR4veEXuwCC2wARtG.jpg  ...  373554033.0    81.0   \n",
              "\n",
              "                               spoken_languages    status  tagline      title  \\\n",
              "id                                                                              \n",
              "862.0  [{'iso_639_1': 'en', 'name': 'English'}]  Released      NaN  Toy Story   \n",
              "\n",
              "       video vote_average vote_count target  \n",
              "id                                           \n",
              "862.0  False          7.7     5415.0      1  \n",
              "\n",
              "[1 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67a22060-7f99-4ef8-8c42-c8c182bc3ad5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adult</th>\n",
              "      <th>budget</th>\n",
              "      <th>genres</th>\n",
              "      <th>homepage</th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>original_language</th>\n",
              "      <th>original_title</th>\n",
              "      <th>overview</th>\n",
              "      <th>popularity</th>\n",
              "      <th>poster_path</th>\n",
              "      <th>...</th>\n",
              "      <th>revenue</th>\n",
              "      <th>runtime</th>\n",
              "      <th>spoken_languages</th>\n",
              "      <th>status</th>\n",
              "      <th>tagline</th>\n",
              "      <th>title</th>\n",
              "      <th>video</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>vote_count</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>862.0</th>\n",
              "      <td>False</td>\n",
              "      <td>30000000.0</td>\n",
              "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
              "      <td>http://toystory.disney.com/toy-story</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>en</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
              "      <td>21.946943</td>\n",
              "      <td>/rhIRbceoE9lR4veEXuwCC2wARtG.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>373554033.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>False</td>\n",
              "      <td>7.7</td>\n",
              "      <td>5415.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67a22060-7f99-4ef8-8c42-c8c182bc3ad5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67a22060-7f99-4ef8-8c42-c8c182bc3ad5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67a22060-7f99-4ef8-8c42-c8c182bc3ad5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xehyn36dKVFS"
      },
      "source": [
        "#### HW 3: Сreate a google folder with experiments, make it a python package, organize all transformers and classifiers wih python modules and .yaml files. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir hw10_exp"
      ],
      "metadata": {
        "id": "PeZa6qrd0qNk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/__init__.py\n",
        "\n",
        "#empty python file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmgZZFVFAdi4",
        "outputId": "ef95cb82-50a5-4396-82ea-62201ec4ffa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw10_exp/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2S0J_4ZKVFT"
      },
      "source": [
        "#### HW 4: Writet agregation and linking code snippets to use infromation from files outside movies_metadata.csv. Store them as functions in experiment structure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/settings.py\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "ROOT_DIR = os.path.join(os.getcwd(), 'hw10_exp') \n",
        "BASE_PARAMS_DIR = os.path.join(ROOT_DIR, 'params.yaml') \n",
        "\n",
        "with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "    cfg = yaml.safe_load(fd)\n",
        "\n",
        "EXPERIMENT_DIR = os.path.join(ROOT_DIR, 'data')\n",
        "request_number = 1\n",
        "PATH = os.path.join(EXPERIMENT_DIR, f'exp{request_number}')\n",
        "\n",
        "while os.path.exists(PATH):\n",
        "    request_number +=1\n",
        "    PATH = os.path.join(EXPERIMENT_DIR, f'exp{request_number}')\n",
        "\n",
        "DATA_PATH = PATH\n",
        "PARAMS_DIR = os.path.join(DATA_PATH, 'experiment_params.yaml')\n",
        "\n",
        "print(ROOT_DIR)\n",
        "print(BASE_PARAMS_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmdxcTzofb6u",
        "outputId": "4cd1dbb5-155a-4ea9-d9d8-f40c0157bc97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw10_exp/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/params.yaml\n",
        "data:\n",
        "  data_path: '/content'\n",
        "  main_data_path: '/content/movies_metadata.csv'       # path to main dataset\n",
        "  joined_data_name: 'None'                             # additional data you want to join to main_data\n",
        "  additional_data: \n",
        "      {\"credits\":'/content/credits.csv',               \n",
        "      \"ratings\":'/content/ratings.csv',\n",
        "      \"ratings_small\":'/content/ratings_small.csv'}\n",
        "  use_categorical: 1\n",
        "  included_features: ['adult',\n",
        "                     'budget',\n",
        "                     'overview', \n",
        "                     'popularity', \n",
        "                     'revenue', \n",
        "                     'runtime',\n",
        "                     'video', \n",
        "                     'vote_count', \n",
        "                     'years', \n",
        "                     'target']\n",
        "experiment:\n",
        "  preprocessing_type: 1\n",
        "  pca: 1                                                    \n",
        "  n_components: 10\n",
        "  random_state: 22\n",
        "  test_size: 0.2\n",
        "  stratify: 1\n",
        "  classifier: 'LogisticRegression' #Choose between\n",
        "params_dectree:\n",
        "    {\"criterion\": 'gini',\n",
        "    \"splitter\": 'best',\n",
        "    \"ccp_alpha\": 0.0}\n",
        "\n",
        "params_logreg:\n",
        "    {\n",
        "    \"penalty\": 'l2',\n",
        "    }\n",
        "\n",
        "params_randomforest:\n",
        "  {\"n_estimators\": 100,}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne0f3czLflEA",
        "outputId": "a0a28509-9a30-4c3f-dedb-38e4c9937ed8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw10_exp/params.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/my_utils.py\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import yaml\n",
        "from hw10_exp.settings import BASE_PARAMS_DIR\n",
        "from hw10_exp.nlp_utils import pos_tokenization\n",
        "from hw10_exp.nlp_utils import lemmatize_df\n",
        "\n",
        "def get_data(BASE_PARAMS_DIR=BASE_PARAMS_DIR):\n",
        "\n",
        "\n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "      cfg = yaml.safe_load(fd)\n",
        "\n",
        "    drop_cols = ['homepage',\n",
        "                  'imdb_id',\n",
        "                  'belongs_to_collection',\n",
        "                  'original_title', \n",
        "                  'spoken_languages', \n",
        "                  'tagline', \n",
        "                  'title',\n",
        "                  'poster_path',           \n",
        "                  'production_companies',  #we might want to use this    \n",
        "                  'production_countries',  #we might want to use this\n",
        "                  'genres',                #we might want to extract data from these jsons\n",
        "                  'release_date',]         #we might wanna keep this, to further create more features based on that\n",
        "      \n",
        "    cat_cols = ['original_language',\n",
        "                  'status']\n",
        "\n",
        "    included_features = cfg.get('data')['included_features']\n",
        "\n",
        "    main_data = pd.read_csv(cfg.get('data')['main_data_path'])\n",
        "    main_data[\"id\"] = pd.to_numeric(main_data[\"id\"], errors=\"coerce\", downcast=\"integer\")\n",
        "    main_data = main_data.drop_duplicates(subset=['id'])\n",
        "    main_data[\"budget\"] = pd.to_numeric(main_data[\"budget\"], errors=\"coerce\")\n",
        "    main_data[\"popularity\"] = pd.to_numeric(main_data[\"popularity\"], errors = \"coerce\")\n",
        "    main_data['overview'] = main_data['overview'].astype(str)\n",
        "\n",
        "    main_data = main_data[~main_data['video'].isna()]\n",
        "    main_data['video'] = main_data['video'].astype(int)\n",
        "    main_data = main_data[main_data['adult'].isin(['True','False'])]\n",
        "    main_data['adult'] = main_data['adult'].map({'True':1,\n",
        "                                                  'False':0})\n",
        "    main_data['years'] = pd.DatetimeIndex(pd.to_datetime(main_data[\"release_date\"], errors=\"coerce\")).year                                               \n",
        "    main_data['years'] = max(main_data['years']) - main_data['years']      #we might wanna consider taking monthes too, \n",
        "                                                                            #and adding 'time of year' feature indicating Summer/Winter and etc...\n",
        "      \n",
        "    main_data = main_data[~main_data.status.isna()]\n",
        "      \n",
        "    main_data = main_data[~main_data['release_date'].isna()]\n",
        "    main_data = main_data[~main_data['years'].isna()]\n",
        "    main_data = main_data.drop(drop_cols, axis=1)\n",
        "    main_data.set_index('id', inplace=True)\n",
        "\n",
        "      #getting median of votes\n",
        "    median_vote = main_data.vote_average.median()\n",
        "\n",
        "      #creating 'target' column\n",
        "    main_data['target'] = (main_data['vote_average']>median_vote).astype(int)\n",
        "    main_data = main_data.drop('vote_average', axis=1)\n",
        "      \n",
        "    if cfg.get('data')['use_categorical'] == 1:\n",
        "      categorical_data = pd.get_dummies(main_data[cat_cols], drop_first=True)\n",
        "      main_data = pd.concat([main_data, categorical_data], axis=1)\n",
        "      \n",
        "    if cfg.get('data')['joined_data_name'] == 'credits':\n",
        "      joined_data = pd.read_csv(cfg.get('data')['additional_data']['credits'])\n",
        "      joined_data.drop_duplicates(subset=\"id\", inplace = True)\n",
        "      main_data = main_data.merge(joined_data, how=\"left\", left_on = \"id\", right_on = \"id\").copy()\n",
        "      \n",
        "    if cfg.get('data')['joined_data_name'] == 'ratings':\n",
        "      joined_data = pd.read_csv(cfg.get('data')['additional_data']['ratings'])\n",
        "      joined_data.drop(['userId','timestamp'], axis=1, inplace=True)\n",
        "      joined_data = joined_data.groupby('movieId').mean()\n",
        "      main_data = main_data.join(joined_data, how='left')\n",
        "\n",
        "    if cfg.get('data')['joined_data_name'] == 'ratings_small':\n",
        "      joined_data = pd.read_csv(cfg.get('data')['additional_data']['ratings_small'])\n",
        "      joined_data = pd.read_csv(cfg.get('data')['additional_data']['ratings'])\n",
        "      joined_data.drop(['userId','timestamp'], axis=1, inplace=True)\n",
        "      joined_data = joined_data.groupby('movieId').mean()\n",
        "      main_data = main_data.join(joined_data, how='left')\n",
        "      \n",
        "    if cfg.get('data')['joined_data_name'] == 'None':\n",
        "      pass\n",
        "        \n",
        "    if (cfg.get('experiment')['preprocessing_type'] == 2):\n",
        "      main_data = pos_tokenization(data=main_data, POS='NN')\n",
        "      \n",
        "    if (cfg.get('experiment')['preprocessing_type'] == 3):\n",
        "      main_data = pos_tokenization(data=main_data, POS='NNP')\n",
        "      \n",
        "      \n",
        "    main_data = main_data.drop(cat_cols, axis=1)\n",
        "    main_data = lemmatize_df(main_data)\n",
        "    main_data['overview'] = main_data['overview'].apply(lambda x: ' '.join(x))\n",
        "    main_data = main_data.dropna(axis='rows')                                   # better idea:  impute/ffil na's\n",
        "    main_data = main_data[included_features]                                   \n",
        "    return main_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxHHJEUWhqRu",
        "outputId": "010a05e5-4811-41e0-9008-35f07b5d072e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw10_exp/my_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/nlp_utils.py\n",
        "import os\n",
        "import yaml\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from collections.abc import Iterable\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "from sklearn.feature_extraction import text as sk_text\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def flatten(lis):\n",
        "     for item in lis:\n",
        "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
        "             for x in flatten(item):\n",
        "                 yield x\n",
        "         else:        \n",
        "             yield item\n",
        "\n",
        "\n",
        "def remove_stopwords(text):   \n",
        "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
        "    text = ' '.join(t)    \n",
        "    return text \n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def remove_irr_char(text):\n",
        "    return re.sub(r'[^a-zA-Z]', ' ', text)    \n",
        "\n",
        "def remove_extra_whitespaces(text):\n",
        "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "def remove_url_func(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "def remove_html_tags_func(text):\n",
        "    return BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_irr_char(text)\n",
        "    text = remove_extra_whitespaces(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    text = remove_url_func(text)\n",
        "    text = remove_html_tags_func(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "def simple_tokenizator(text):\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    return(text)\n",
        "\n",
        "def pos_tokenization(data, POS):\n",
        "    print(f'Collecting n-grams for {POS} tokes')\n",
        "    data = data.copy()\n",
        "    data['overview'] = data['overview'].map(sent_tokenize)\n",
        "    data['overview'] = data['overview'].apply(lambda x: [word_tokenize(sentence) for sentence in x])\n",
        "    data['overview'] = data['overview'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
        "    data['overview'] = data['overview'].apply(\n",
        "      lambda x: [\n",
        "                [\n",
        "                    word_POS[0] for word_POS in sentence if (word_POS[1] == POS)\n",
        "                ] \n",
        "                for sentence in x\n",
        "          ]\n",
        "      )\n",
        "    data['overview'] = data['overview'].apply(lambda x: ' '.join(list(flatten(x))))\n",
        "    data['overview'] = data['overview'].str.lower()\n",
        "    return data\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    \n",
        "    return [lemmatizer.lemmatize(w.lower()) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "def lemmatize_df(data):\n",
        "    data = data.copy()\n",
        "    data['overview'] = data.overview.apply(lemmatize_text)\n",
        "\n",
        "    return data\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fBW0Nlw_lLB",
        "outputId": "ee245da8-682f-460f-a0fc-2349ebae44f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hw10_exp/nlp_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBMtIHiiKVFT"
      },
      "source": [
        "#### HW 5: Create set of classifiers for linear regression with different features for feature generation. \n",
        "use the following methods of textual feature generation for overview features\n",
        "\n",
        " - 1 word tokenization with 1-gram\n",
        " - 2 pos tokenization with extraction of all NP #did this for NN \n",
        " - 3 pos tokenization with 2-gram bag of tokens for all NNP\n",
        " - 4 2-gram bag of characters for overview features\n",
        " - calculate frequencies for obtained tokens/ngrams and use as a feature\n",
        " \n",
        "Organize classes of transformers in various .py files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/data_transforms.py\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hw10_exp.nlp_utils import pos_tokenization\n",
        "\n",
        "\n",
        "\n",
        "class Hw5_1Transform(TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    X = X.copy()                            \n",
        "    self.vectorizer = TfidfVectorizer(min_df=20,\n",
        "                                      stop_words='english')\n",
        "    self.vectorizer.fit(X.overview)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X = X.copy()                                 \n",
        "    transformed_features = self.vectorizer.transform(X.overview)\n",
        "    transformed_features = pd.DataFrame(transformed_features.toarray(),\n",
        "                                        columns=self.vectorizer.get_feature_names(),\n",
        "                                        index= X.index)\n",
        "    X = X.drop('overview',axis=1)\n",
        "    X = X.join(transformed_features, rsuffix='_feature')\n",
        "    return X\n",
        "\n",
        "  def fit_transform(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    self.fit(X, y=None)\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "\n",
        "\n",
        "class Hw5_2Transform(TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    X = X.copy()                            \n",
        "    self.vectorizer = TfidfVectorizer(min_df=10,\n",
        "                                      stop_words='english')\n",
        "    self.vectorizer.fit(X.overview)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X = X.copy()                                 \n",
        "    transformed_features = self.vectorizer.transform(X.overview)\n",
        "    transformed_features = pd.DataFrame(transformed_features.toarray(),\n",
        "                                        columns=self.vectorizer.get_feature_names(),\n",
        "                                        index= X.index)\n",
        "    X = X.join(transformed_features, rsuffix='_feature')\n",
        "    X = X.drop('overview',axis=1)\n",
        "    return X\n",
        "    \n",
        "  def fit_transform(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    self.fit(X, y=None)\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "class Hw5_3Transform(TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    self.vectorizer = TfidfVectorizer(min_df=10, \n",
        "                                      stop_words='english',\n",
        "                                      ngram_range=(2,2))\n",
        "    self.vectorizer.fit(X.overview)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    transformed_features = self.vectorizer.transform(X.overview)\n",
        "    transformed_features = pd.DataFrame(transformed_features.toarray(),\n",
        "                                        columns=self.vectorizer.get_feature_names(),\n",
        "                                        index= X.index)\n",
        "    X = X.join(transformed_features, rsuffix='_feature')\n",
        "    X = X.drop('overview',axis=1)\n",
        "    return X\n",
        "    \n",
        "  def fit_transform(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    self.fit(X, y=None)\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "\n",
        "class Hw5_4Transform(TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    X = X.copy()                            \n",
        "    self.vectorizer = TfidfVectorizer(min_df=10,\n",
        "                                      stop_words='english',\n",
        "                                      analyzer='char',\n",
        "                                      ngram_range=(2,2))\n",
        "    self.vectorizer.fit(X.overview)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X = X.copy()                                 \n",
        "    transformed_features = self.vectorizer.transform(X.overview)\n",
        "    transformed_features = pd.DataFrame(transformed_features.toarray(),\n",
        "                                        columns=self.vectorizer.get_feature_names(),\n",
        "                                        index= X.index)\n",
        "    X = X.join(transformed_features, rsuffix='_feature')\n",
        "    X = X.drop('overview',axis=1)\n",
        "    return X\n",
        "\n",
        "  def fit_transform(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    self.fit(X, y=None)\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    X = X.copy()\n",
        "    X = self.transform(X, y=None)\n",
        "    return X\n",
        "\n",
        "class PCA_transform(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, condition, transformer):\n",
        "    self.condition = condition\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    if self.condition:\n",
        "      self.transformer.fit(X, y)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    if self.condition:\n",
        "      X = self.transformer.transform(X)\n",
        "    return X\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czWGk4CstIF2",
        "outputId": "0960e284-4650-4b76-9fc6-ab7b75358b11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hw10_exp/data_transforms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE-NT52DKVFT"
      },
      "source": [
        "#### HW 6: Apply PCA dimensionality reduction and LogisticRegression to predict Target, construct pipelines for all transformers from HW 5 (so it will be 5 various pipeline, name them exp_hw6_1, ... exp_hw6_5) implement them as custom classifiers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added conditional 'pca'. if pass pca=True in make_pipe(), we will generate pipeline with pca"
      ],
      "metadata": {
        "id": "BX1UMfU4JpjS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qNn8vkBKVFU"
      },
      "source": [
        "#### HW 7: Split dataset to train and test (it is up to you which features you will include in it) and store it data folder along with .yaml description"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir hw10_exp/data"
      ],
      "metadata": {
        "id": "bgmIHaJgssSv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/custom_pipeline.py\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from hw10_exp.my_utils import get_data\n",
        "from hw10_exp.settings import ROOT_DIR, BASE_PARAMS_DIR, EXPERIMENT_DIR, PATH, DATA_PATH, PARAMS_DIR\n",
        "from hw10_exp.data_transforms import Hw5_1Transform, Hw5_2Transform, Hw5_3Transform, Hw5_4Transform, PCA_transform\n",
        "\n",
        "def make_pipeline(BASE_PARAMS_DIR=BASE_PARAMS_DIR, pipe_type=None, pca=None, n_components=None, ):\n",
        "\n",
        "  with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "    cfg = yaml.safe_load(fd)\n",
        "\n",
        "  pca = cfg.get('experiment')['pca']\n",
        "  n_components = cfg.get('experiment')['n_components']\n",
        "  pipe_type = cfg.get('experiment')['preprocessing_type']\n",
        "  random_state =  cfg.get('experiment')['random_state']\n",
        "  isOverview = 'overview' in cfg.get('data')['included_features']\n",
        "  \n",
        "  clf_type = cfg.get('experiment')['classifier']\n",
        "  \n",
        "\n",
        "  if clf_type == 'LogisticRegression':\n",
        "    params = cfg.get('params_logreg')\n",
        "    clf = LogisticRegression(**params,random_state=random_state)\n",
        "    \n",
        "  if clf_type == 'DecisionTree':\n",
        "    params = cfg.get('params_dectree')\n",
        "    clf = DecisionTreeClassifier(**params, random_state=random_state)\n",
        "  \n",
        "  if clf_type =='RandomForest':\n",
        "    params = cfg.get('params_randomforest')\n",
        "    clf = RandomForestClassifier(**params, random_state=random_state)\n",
        "\n",
        "  else:\n",
        "    clf = LogisticRegression(random_state=random_state)\n",
        "\n",
        "  if not isOverview:\n",
        "    pipe = Pipeline([\n",
        "    ('standard-scaler', StandardScaler()),\n",
        "    ('PCA', PCA_transform(condition=pca, transformer=PCA(n_components=n_components))),\n",
        "    ('clf', clf),\n",
        "    ])\n",
        "\n",
        "  if (pipe_type == 1) and (isOverview):\n",
        "    pipe = Pipeline([\n",
        "    ('data-transforms', Hw5_1Transform()),\n",
        "    ('standard-scaler', StandardScaler()),\n",
        "    ('PCA', PCA_transform(condition=pca, transformer=PCA(n_components=n_components))),\n",
        "    ('clf', clf),\n",
        "    ])    \n",
        "  if (pipe_type == 2) and (isOverview):\n",
        "    pipe = Pipeline([\n",
        "    ('data-transforms', Hw5_2Transform()),\n",
        "    ('standard-scaler', StandardScaler()),\n",
        "    ('PCA', PCA_transform(condition=pca, transformer=PCA(n_components=n_components))),\n",
        "    ('clf', clf),\n",
        "    ])\n",
        "\n",
        "  if (pipe_type == 3) and (isOverview):\n",
        "    pipe = Pipeline([\n",
        "    ('data-transforms', Hw5_3Transform()),\n",
        "    ('standard-scaler', StandardScaler()),\n",
        "    ('PCA', PCA_transform(condition=pca, transformer=PCA(n_components=n_components))),\n",
        "    ('clf', clf),\n",
        "    ])\n",
        "\n",
        "  if (pipe_type == 4) and (isOverview):  \n",
        "    pipe = Pipeline([\n",
        "    ('data-transforms', Hw5_4Transform()),\n",
        "    ('standard-scaler', StandardScaler()),\n",
        "    ('PCA', PCA_transform(condition=pca, transformer=PCA(n_components=n_components))),\n",
        "    ('clf', clf),\n",
        "    ])    \n",
        "  return pipe, params\n",
        "\n",
        "def split_data(data,\n",
        "               BASE_PARAMS_DIR=BASE_PARAMS_DIR,\n",
        "               EXPERIMENT_DIR=EXPERIMENT_DIR,\n",
        "               PATH=PATH,\n",
        "               DATA_PATH=DATA_PATH,\n",
        "               PARAMS_DIR=PARAMS_DIR,):\n",
        "    \n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "      cfg = yaml.safe_load(fd)\n",
        "    \n",
        "\n",
        "  \n",
        "    data = data.copy()\n",
        "  \n",
        "    train_data, val_data = train_test_split(data, \n",
        "                                             test_size=cfg.get('experiment')['test_size'],\n",
        "                                             random_state=cfg.get('experiment')['random_state'],\n",
        "                                             stratify=data.target if cfg.get('experiment')['stratify'] else None\n",
        "                                             )\n",
        "   \n",
        "\n",
        "    try:\n",
        "      os.makedirs(DATA_PATH, exist_ok=True)\n",
        "    except FileExistsError:\n",
        "      pass\n",
        "\n",
        "    train_data.to_csv(DATA_PATH + '/train_data.csv',)\n",
        "    val_data.to_csv(DATA_PATH+'/val_data.csv',)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v53B0gK9X9vI",
        "outputId": "b11e4c3b-f89b-476d-9424-ca113659b4d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hw10_exp/custom_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7vDCZz8KVFU"
      },
      "source": [
        "#### HW 8: Add intialization from .yaml descriptions of classifiers to implementations of classifiers at HW 6. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# added in custom_pipeline.py With params.yaml one can choose which classifier to use"
      ],
      "metadata": {
        "id": "26oKP9tlf8y8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3VPXJzHKVFU"
      },
      "source": [
        "#### HW 9: Train classifiers with various PCA dimensionality, bag of words and polynomial paramters paramteres on train, test them on test and store in .yaml files for every experiment with resulting metrics\n",
        "\n",
        "- accuracy\n",
        "- precision\n",
        "- recall\n",
        "- adjusted_mutual_information"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw10_exp/train.py\n",
        "import yaml\n",
        "import os\n",
        "from hw10_exp.settings import ROOT_DIR, BASE_PARAMS_DIR, EXPERIMENT_DIR, PATH, DATA_PATH, PARAMS_DIR\n",
        "from hw10_exp.custom_pipeline import split_data, make_pipeline\n",
        "from hw10_exp.my_utils import get_data\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score, adjusted_mutual_info_score, roc_auc_score, log_loss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "def train_clf():\n",
        "    with open(BASE_PARAMS_DIR,'r') as fd:\n",
        "      cfg = yaml.safe_load(fd)\n",
        "\n",
        "\n",
        "\n",
        "    TRAIN_PATH = DATA_PATH +'/train_data.csv'\n",
        "    VAL_PATH = DATA_PATH +'/val_data.csv'\n",
        "    \n",
        "    if not ((os.path.exists(TRAIN_PATH) and (os.path.exists(VAL_PATH)))):\n",
        "      split_data(get_data())\n",
        "    \n",
        "\n",
        "    train_data = pd.read_csv(TRAIN_PATH).dropna(axis='rows')   \n",
        "    val_data = pd.read_csv(VAL_PATH).dropna(axis='rows')\n",
        "\n",
        "    X_train, y_train = train_data.drop(['id','target'],axis=1), train_data.target   \n",
        "    X_val, y_val = val_data.drop(['id','target'],axis=1), val_data.target\n",
        "    \n",
        "    pipe, params = make_pipeline()\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    preds = pipe.predict(X_val)\n",
        "    preds_proba = pipe.predict_proba(X_val)\n",
        "\n",
        "    metrics_dict = {\n",
        "    'precision_score': float(precision_score(y_pred=preds, y_true=y_val)),\n",
        "    'recall_score': float(recall_score(y_pred=preds, y_true=y_val)),\n",
        "    'accuracy_score': float(accuracy_score(y_pred=preds, y_true=y_val)),\n",
        "    'adjusted_mutual_info_score': float(adjusted_mutual_info_score(y_val, preds)),\n",
        "    'roc_auc_score': float(roc_auc_score(y_val, preds_proba[:,1])),\n",
        "    'log_loss': float(log_loss(y_val, preds_proba[:,1])),\n",
        "    }\n",
        "\n",
        "\n",
        "    with open(DATA_PATH + '/report.yaml', 'w') as f:\n",
        "      yaml.dump(metrics_dict, f, default_flow_style=False)\n",
        "\n",
        "    joblib.dump(pipe, DATA_PATH + '/model.pkl')\n",
        "\n",
        "\n",
        "    with open(PARAMS_DIR, 'w') as f:\n",
        "      documents = yaml.dump(cfg, f)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS_tVhRXgDtg",
        "outputId": "0cc188b1-bedb-4dee-f397-9772fbebe37c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hw10_exp/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sItXwf71KVFV"
      },
      "source": [
        "#### HW 10*: Invent as many ways to use additional files as possible, particularly try to aquire poster images and analyze them with computer vision methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-z4PGeKVFV"
      },
      "source": [
        "#### HW 11: Add new features to all PCA output on HW 6, train classifiers, test quality and store results in proper .yaml files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** I have solved this. PCA_transform class is written in data_transforms.py, and we can add/exclude PCA features in main params.yaml file ('experiment', 'pca', 'n_components')**"
      ],
      "metadata": {
        "id": "8IShAhsiD6dc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx56ahDiKVFW"
      },
      "source": [
        "#### HW 12*: Perform topic modelings on text feature, add it to PCA features from HW 6, train classifiers, test quality and store results in proper .yaml files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t23JCuXmKVFW"
      },
      "source": [
        "#### HW 13: Combine all combinations of features from HW6, measure the results of improvement of classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6uIqDBKVFW"
      },
      "source": [
        "#### HW 14*: For 5 best of prevous experiments, change LinearRegression to XGBClassifier. Try several XGB configurations, store results and parameters in .yaml files"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sItXwf71KVFV",
        "Bx56ahDiKVFW",
        "t23JCuXmKVFW"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}